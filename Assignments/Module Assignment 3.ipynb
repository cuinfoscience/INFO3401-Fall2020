{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3401 â€“ Module Assignment 3\n",
    "\n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT).\n",
    "\n",
    "## Learning Objectives\n",
    "This is one of two required sub-assignments for Module Assignment 3. In this assignment we want you to analyze time series data about a Wikipedia article. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our usual libraries for working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Our usual libraries for visualizing data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Some new libraries for retrieving data from the web, working with time, etc.\n",
    "import requests, json\n",
    "from datetime import datetime\n",
    "from urllib.parse import unquote, quote\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two functions to start\n",
    "\n",
    "Below are two functions that I use in my research to retrieve data about Wikipedia articles. While Module 6 on \"Structured Data\" at the end of the term will go into more detail, you already have all the knowledge from INFO 1201 and 2201 to be able to interpret what these functions are doing. That said: you do not need to edit anything about these functions and can treat them like black boxes and you can skip down to \"Step 0\" if you're genuinely uncurious.\n",
    "\n",
    "### Get the revision history for an article\n",
    "\n",
    "Wikipedia keeps track of every single change made to every single article going back to approximately 2002. This is called a revision history: [this is the revision history for the \"University of Colorado Boulder\" article](https://en.wikipedia.org/w/index.php?title=University_of_Colorado_Boulder&action=history). You could get the content from previous versions of articles if you wanted from the API (again, check back in for Module 6) but this function simply retrieves the metadata about every revision: the ID of the revision, the ID of the user, if the user left a comment, the timestamp of the change, the username, the size of the page, and a SHA1 hash which is helpful for comparing if two versions of an article are identical.\n",
    "\n",
    "I would **very** strongly caution you against trying to retrieve the complete revision histories for popular topics like \"Donald Trump\", \"Hillary Clinton\", etc. This function will work, but because these can have tens of thousands of revisions, it could take several minutes to retrieve them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_revisions(json_response):\n",
    "    if type(json_response['query']['pages']) == dict:\n",
    "        page_id = list(json_response['query']['pages'].keys())[0]\n",
    "        return json_response['query']['pages'][page_id]['revisions']\n",
    "    elif type(json_response['query']['pages']) == list:\n",
    "        if 'revisions' in json_response['query']['pages'][0]:\n",
    "            return json_response['query']['pages'][0]['revisions']\n",
    "        else:\n",
    "            return list()\n",
    "    else:\n",
    "        raise ValueError(\"There are no revisions in the JSON\")\n",
    "\n",
    "def get_all_page_revisions(page_title, endpoint='en.wikipedia.org/w/api.php', redirects=1):\n",
    "    \"\"\"Takes Wikipedia page title and returns a DataFrame of revisions\n",
    "    \n",
    "    page_title - a string with the title of the page on Wikipedia\n",
    "    endpoint - a string that points to the web address of the API.\n",
    "        This defaults to the English Wikipedia endpoint: 'en.wikipedia.org/w/api.php'\n",
    "        Changing the two letter language code will return a different language edition\n",
    "        The Wikia endpoints are slightly different, e.g. 'starwars.wikia.com/api.php'\n",
    "    redirects - a Boolean value for whether to follow redirects to another page\n",
    "        \n",
    "    Returns:\n",
    "    df - a pandas DataFrame where each row is a revision and columns correspond\n",
    "         to meta-data such as parentid, revid, sha1, size, timestamp, and user name\n",
    "    \"\"\"\n",
    "    \n",
    "    # A container to store all the revisions\n",
    "    revision_list = list()\n",
    "    \n",
    "    # Set up the query\n",
    "    query_url = \"https://{0}\".format(endpoint)\n",
    "    query_params = {}\n",
    "    query_params['action'] = 'query'\n",
    "    query_params['titles'] = page_title\n",
    "    query_params['prop'] = 'revisions'\n",
    "    query_params['rvprop'] = 'ids|userid|comment|timestamp|user|size|sha1'\n",
    "    query_params['rvlimit'] = 500\n",
    "    query_params['rvdir'] = 'newer'\n",
    "    query_params['format'] = 'json'\n",
    "    query_params['redirects'] = redirects\n",
    "    query_params['formatversion'] = 2\n",
    "    \n",
    "    # Make the query\n",
    "    json_response = requests.get(url = query_url, params = query_params).json()\n",
    "\n",
    "    # Add the temporary list to the parent list\n",
    "    revision_list += response_to_revisions(json_response)\n",
    "\n",
    "    # Loop for the rest of the revisions\n",
    "    while True:\n",
    "\n",
    "        # Newer versions of the API return paginated results this way\n",
    "        if 'continue' in json_response:\n",
    "            query_continue_params = deepcopy(query_params)\n",
    "            query_continue_params['rvcontinue'] = json_response['continue']['rvcontinue']\n",
    "            json_response = requests.get(url = query_url, params = query_continue_params).json()\n",
    "            revision_list += response_to_revisions(json_response)\n",
    "        \n",
    "        # Older versions of the API return paginated results this way\n",
    "        elif 'query-continue' in json_response:\n",
    "            query_continue_params = deepcopy(query_params)\n",
    "            query_continue_params['rvstartid'] = json_response['query-continue']['revisions']['rvstartid']\n",
    "            json_response = requests.get(url = query_url, params = query_continue_params).json()\n",
    "            revision_list += response_to_revisions(json_response)\n",
    "        \n",
    "        # If there are no more revisions, stop\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(revision_list)\n",
    "\n",
    "    # Add in some helpful fields to the DataFrame\n",
    "    df['page'] = json_response['query']['pages'][0]['title']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pageviews for an article\n",
    "\n",
    "Wikipedia also keeps track of all the times an article was accessed, ***but only back to July 2015***. [Here are the recent page views for the \"University of Colorado Boulder\" article](https://pageviews.toolforge.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=latest-20&pages=University_of_Colorado_Boulder). Getting all five years of data should take longer than a few months, so please keep that in mind, but there's no penalty for getting the pageview data for a popular topics compared to a non-popular topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pageviews(page_title,endpoint='en.wikipedia.org',date_from='20150701',date_to='today'):\n",
    "    \"\"\"Takes Wikipedia page title and returns a all the various pageview records\n",
    "    \n",
    "    page_title - a string with the title of the page on Wikipedia\n",
    "    lang - a string (typically two letter ISO 639-1 code) for the language edition,\n",
    "        defaults to \"en\"\n",
    "        datefrom - a date string in a YYYYMMDD format, defaults to 20150701 (earliest date)\n",
    "        dateto - a date string in a YYYYMMDD format, defaults to today\n",
    "        \n",
    "    Returns:\n",
    "    s - a Series indexed by date with page views as values\n",
    "    \"\"\"\n",
    "    if date_to == 'today':\n",
    "        date_to = str(datetime.today().date()).replace('-','')\n",
    "        \n",
    "    quoted_page_title = quote(page_title, safe='')\n",
    "    date_from = datetime.strftime(pd.to_datetime(date_from),'%Y%m%d')\n",
    "    date_to = datetime.strftime(pd.to_datetime(date_to),'%Y%m%d')\n",
    "    \n",
    "    s = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{1}/{2}/{3}/{0}/daily/{4}/{5}\".format(quoted_page_title,endpoint,'all-access','user',date_from,date_to)\n",
    "    json_response = requests.get(s).json()\n",
    "    \n",
    "    if 'items' in json_response:\n",
    "        df = pd.DataFrame(json_response['items'])\n",
    "    else:\n",
    "        raise KeyError('There is no \"items\" key in the JSON response.')\n",
    "        \n",
    "    df = df[['timestamp','views']]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],format='%Y%m%d%H')\n",
    "    s = df.set_index('timestamp')['views']\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Write down a research question and motivation\n",
    "\n",
    "Why are you choosing this article to analyze? What kinds of information production (revisions) and consumption (pageviews) behavior are you expecting to observe? For whom does this matter and why? Because of the limitations of the data, try to come up with questions/motivations/hypotheses for behavior in or after July 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve the revision history for a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Use the `get_all_page_revisions` function on your article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cub_rev_df = get_all_page_revisions('University of Colorado Boulder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Inspect the top and print out the number of revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1c: When was the first revision made? The most recent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1d: What \"user\" made the most revisions to this article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Engineer new temporal features\n",
    "\n",
    "We want to make several new columns that capture temporal behavior.\n",
    "\n",
    "### Step 2a: Convert the \"timestamp\" column from `str` to `pd.Timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Create a column \"date\" that has only the dates of the revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c: Create a column \"weekday\" that has the day of the week of the revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2d: Create a column \"hour\" that has the hour of the revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2e: Create a column \"diff\" that is the change in the \"size\" since the last revision\n",
    "\n",
    "This is not a temporal variable but it will require you to use a method we discussed in the pre-class lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2f: Create a column \"lag\" that has the time elapsed since the previous edit\n",
    "\n",
    "Make sure this is stored as a float or an integer of a reasonable unit (minutes, hours, days, weeks, etc.) and **not as a Timedelta**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2g: Create a column \"age\" that has the time elapsed since the first edit\n",
    "\n",
    "Make sure this is stored as a float or an integer of a reasonable unit (minutes, hours, days, weeks, etc.) and **not as a Timedelta**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the revision data\n",
    "\n",
    "### Step 3a: Use the \"hour\" and \"weekday\" columns to make a revision heatmap\n",
    "\n",
    "Use your data reshaping skills like `pivot_table` and read up on how to make a [heatmap in Seaborn](https://seaborn.pydata.org/generated/seaborn.heatmap.html) ([also](https://towardsdatascience.com/heatmap-basics-with-pythons-seaborn-fb92ea280a6c)) showing when revisions on your article tend to happen. Your revision heatmap should have \"weekday\" as columns and \"hour\" as an index and a count of revisions as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Interpret and summarize some interesting features about the revision heatmap\n",
    "\n",
    "During what part of the day and week are most revisions to this article made? Is this surprising or expected? What kinds of mechanisms social conventions for time could explain why there is or is not a clear pattern? If you had to tell someone to give you a report on the state of the article every week, when should they look at the article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c: Make a line plot of the number of revisions over time\n",
    "\n",
    "Use pandas's groupby, reindex, and/or resample functionality to count the number of revisions by day. Make sure that the date range is continuous without gaps and dates without revision activity have an appropriate value. Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d: Interpret and summarize some interesting features about the revision activity line plot\n",
    "\n",
    "Are there any trends of increasing or decreasing revision activity? Are there any instances of \"bursts\" of revision activity? If so, do these bursts correspond to meaningful events in the \"real\" world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3e: Make a scatter plot of \"lag\" and \"diff\"\n",
    "\n",
    "Make a scatterplot with the \"lag\" column on the x-axis and the \"diff\" column on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3f: Interpret and summarize some interesting features about the lag-diff scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve the pageviews for a single article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4a: Use the `get_pageviews` function on your article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cub_pv_df = get_pageviews('University of Colorado Boulder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b: Inspect the top and print out the number of pageview observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4c: Make sure the time variables are `Timestamp`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4d: What date had the most pageviews? The fewest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize pageview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Make a line plot of the number of revisions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Make a barplot of the pageview activity by day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5c: Make an autocorrelation plot of the pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5d: Interpret and summarize some interesting features about the pageview behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Combing revision and pageview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6a: Make a DataFrame containing only daily revision and pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6b: Plot both these time series on the same figure\n",
    "\n",
    "Use good visualization practices to ensure that a reader can identify meaningful variability in both. See using [Scales](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#scales), [Plotting on a secondary y-axis](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#plotting-on-a-secondary-y-axis), or [Subplots](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#subplots) for ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6c: Interpret and summarize some interesting features about revision and pageview behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6d: Revisit your research question/hypotheses/motivation and discuss in light of your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
